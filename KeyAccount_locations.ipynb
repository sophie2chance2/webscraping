{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = {'CREDENTIALS_HERE'}\n",
    "gc = gspread.service_account_from_dict(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# franchise_wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1inma86cQKoPFBZX2txtj6oXqL3VYN58eT0frUTdLRQs/edit#gid=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AVEDA ###\n",
    "\n",
    "url = 'https://aveda.edu/locations/'\n",
    "req = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "all_locations = soup.find_all('div', class_='location-tile')\n",
    "\n",
    "aveda_dict = {}\n",
    "aveda_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(all_locations)):\n",
    "    aveda_dict['Website'] = all_locations[i].find(href=True)['href']\n",
    "    state = all_locations[i].find('div', class_='label').text\n",
    "    aveda_dict['State'] = state\n",
    "    city = all_locations[i].find('span', class_='city').text\n",
    "    aveda_dict['City'] = city\n",
    "    # aveda_dict['Phone'] = all_locations[i].find('span', class_='phone').text\n",
    "    aveda_dict['Address'] = all_locations[i].find('span', class_='address').text + ' - ' + city + ', ' + state\n",
    "    school_name = all_locations[i].find('span', class_='school').text\n",
    "    if city in school_name:\n",
    "        aveda_dict['School Name'] = school_name.replace(city, ' - '+ city).replace('  ',' ').replace(' - - ', ' - ').replace('(', '').replace(')', '')\n",
    "    else:\n",
    "        aveda_dict['School Name'] = school_name + ' - ' + city\n",
    "\n",
    "    aveda_df = aveda_df.append(aveda_dict, ignore_index=True)\n",
    "\n",
    "aveda_df = aveda_df.reindex(['Company ID', 'School Name', 'City', 'State', 'Address', 'Website'], axis=1)\n",
    "\n",
    "aveda_sheet = franchise_wb.worksheet('Aveda')\n",
    "set_with_dataframe(aveda_sheet, aveda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summit Salon Academy ###\n",
    "\n",
    "url = 'https://summitsalonacademy.com/academies/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "summit_salon_dict = {}\n",
    "summit_salon_df = pd.DataFrame()\n",
    "\n",
    "all_locations = soup.find_all('div', class_='et_pb_blurb_content')\n",
    "\n",
    "for i in range(len(all_locations)):\n",
    "    this_location = BeautifulSoup(str(all_locations[i]))\n",
    "\n",
    "    city = this_location.find(class_='et_pb_module_header').text.split(', ')[0].title()\n",
    "    summit_salon_dict['City'] = city\n",
    "    summit_salon_dict['State'] = this_location.find(class_='et_pb_module_header').text.split(', ')[1]\n",
    "    summit_salon_dict['Programs'] = str(this_location.find_all('li')).replace('<li>', '').replace('</li>', '')\n",
    "    summit_salon_dict['Website'] = this_location.find(href=True)['href']\n",
    "    summit_salon_dict['Name'] = 'Summit Salon Academy - ' + city\n",
    "\n",
    "    summit_salon_df = summit_salon_df.append(summit_salon_dict, ignore_index=True)\n",
    "\n",
    "summit_salon_df = summit_salon_df.reindex(['Company ID', 'Name', 'City', 'State', 'Programs', 'Website'], axis=1)\n",
    "\n",
    "salon_summit = franchise_wb.worksheet('Summit Salon')\n",
    "set_with_dataframe(salon_summit, summit_salon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Toni & Guy ###\n",
    "\n",
    "url = 'https://careerschoolnow.org/colleges/toni-guy-hairdressing-academy'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "toni_guy_dict = {}\n",
    "toni_guy_df = pd.DataFrame()\n",
    "\n",
    "all_locations = soup.find_all('div', class_='campus-listing')\n",
    "\n",
    "for i in range(len(all_locations)):\n",
    "    this_location = all_locations[i]\n",
    "\n",
    "    toni_guy_dict['Website'] = this_location.find(href=True)['href']\n",
    "    toni_guy_dict['Name'] = this_location.find(href=True)['title'].replace('-', ' - ')\n",
    "    toni_guy_dict['Address'] = str(this_location.find('p', class_='full_address')).replace('<p class=\"full_address\">', '').replace('</p>', '').replace('<br/>', ' - ')\n",
    "    toni_guy_dict['City'] = str(this_location.find('p', class_='full_address')).replace('<p class=\"full_address\">', '').replace('</p>', '').split('<br/>')[1].split(', ')[0]\n",
    "    state = str(this_location.find('p', class_='full_address')).replace('<p class=\"full_address\">', '').replace('</p>', '').split('<br/>')[1].split(', ')[1]\n",
    "    toni_guy_dict['State'] = re.sub('\\d', '', state)\n",
    "\n",
    "    toni_guy_df = toni_guy_df.append(toni_guy_dict, ignore_index = True)\n",
    "\n",
    "toni_guy_df = toni_guy_df.reindex(['Company ID', 'Name', 'Address', 'City', 'State', 'Website'], axis=1)\n",
    "\n",
    "toni_guy = franchise_wb.worksheet('Toni&Guy')\n",
    "set_with_dataframe(toni_guy, toni_guy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPEC Franchise ###\n",
    "\n",
    "url = 'https://www.specfranchise.com/about-us/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "spec_dict = {}\n",
    "spec_df = pd.DataFrame()\n",
    "\n",
    "all_locations = soup.find_all('div', class_='shcool-info-content')\n",
    "\n",
    "for i in range(len(all_locations)):\n",
    "    this_location = all_locations[i]\n",
    "\n",
    "    name = this_location.find('div', class_='site-notice').text.replace('\\n', '').replace('  ', '').strip().replace('TSPA ', 'TSPA - ').replace('ESI ', 'ESI - ')\n",
    "    spec_dict['Name'] = name\n",
    "    address = this_location.find('div', class_='address').text.replace('Location: ', '').replace(name +', ', '').replace(', USA', '')\n",
    "    spec_dict['City'] = name.replace('TSPA - ', '').replace('ESI - ', '')\n",
    "    spec_dict['Address'] = address\n",
    "    spec_dict['State'] = re.sub('\\d', '', address.split(', ')[-1]).replace(' ', '')\n",
    "    spec_dict['Website'] = this_location.find(href=True)['href']\n",
    "\n",
    "    spec_df = spec_df.append(spec_dict, ignore_index = True)\n",
    "\n",
    "spec_df = spec_df.reindex(['Company ID', 'Name', 'Address', 'City', 'State', 'Website'], axis=1)\n",
    "\n",
    "spec_sheet = franchise_wb.worksheet('SPEC')\n",
    "set_with_dataframe(spec_sheet, spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRICOCI ###\n",
    "url = 'https://www.tricociuniversity.edu/cosmetology-esthetics-barbering-schools/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "tricoci_dict = {}\n",
    "tricoci_df = pd.DataFrame()\n",
    "\n",
    "illinois = soup.find_all('ul', id='menu-illinois-side-navigation')\n",
    "illinois_schools = BeautifulSoup(str(illinois)).find_all('li')\n",
    "\n",
    "for i in range(len(illinois_schools)):\n",
    "    this_location = illinois_schools[i]\n",
    "\n",
    "    tricoci_dict['Name'] = 'Tricoci - ' + this_location.text\n",
    "    tricoci_dict['Website'] = 'tricociuniversity.edu' + this_location.find(href=True)['href']\n",
    "    tricoci_dict['City'] = this_location.text\n",
    "    tricoci_dict['State'] = 'Illinois'\n",
    "\n",
    "    tricoci_df = tricoci_df.append(tricoci_dict, ignore_index = True)\n",
    "\n",
    "indiana = soup.find_all('ul', id='menu-indiana-side-navigation')\n",
    "indiana_schools = BeautifulSoup(str(indiana)).find_all('li')\n",
    "\n",
    "for i in range(len(indiana_schools)):\n",
    "    this_location = indiana_schools[i]\n",
    "\n",
    "    tricoci_dict['Name'] = 'Tricoci - ' + this_location.text\n",
    "    tricoci_dict['Website'] = 'tricociuniversity.edu' + this_location.find(href=True)['href']\n",
    "    tricoci_dict['City'] = this_location.text\n",
    "    tricoci_dict['State'] = 'Indiana'\n",
    "\n",
    "    tricoci_df = tricoci_df.append(tricoci_dict, ignore_index = True)\n",
    "\n",
    "wisconsin = soup.find_all('ul', id='menu-wisconsin-side-navigation')\n",
    "wisconsin_schools = BeautifulSoup(str(wisconsin)).find_all('li')\n",
    "\n",
    "for i in range(len(wisconsin_schools)):\n",
    "    this_location = wisconsin_schools[i]\n",
    "\n",
    "    tricoci_dict['Name'] = 'Tricoci - ' + this_location.text\n",
    "    tricoci_dict['Website'] = 'tricociuniversity.edu' + this_location.find(href=True)['href']\n",
    "    tricoci_dict['City'] = this_location.text\n",
    "    tricoci_dict['State'] = 'Wisconsin'\n",
    "\n",
    "    tricoci_df = tricoci_df.append(tricoci_dict, ignore_index = True)\n",
    "\n",
    "tricoci_df = tricoci_df.reindex(['Company ID', 'Name', 'City', 'State', 'Website'], axis=1)\n",
    "\n",
    "tricoci_sheet = franchise_wb.worksheet('Tricoci')\n",
    "set_with_dataframe(tricoci_sheet, tricoci_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.specfranchise.com/about-us/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import urllib3 \n",
    "from base64 import encode \n",
    " \n",
    " \n",
    "http = urllib3.PoolManager() \n",
    " \n",
    "user = 'EMAIL'\n",
    "passwd = 'PASSWORD'\n",
    "hash_string = f\"{user}:{passwd}\"\n",
    "hash_string = hash_string.encode(encoding='UTF-8',errors='strict')\n",
    "headers = { \n",
    "\"Authorization\": f\"Basic {hash_string}\" \n",
    "} \n",
    " \n",
    "url = 'https://sa1.connectandsell.com/ConnectAndSell/'\n",
    "# Make your request \n",
    "req = urllib.request.Request(url, headers=headers)\n",
    "response = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(response)\n",
    "\n",
    "# Convert JSON to dict \n",
    "# response_data = json.loads(response.data.decode(\"utf-8\")) \n",
    " \n",
    "# print(response_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ac91f7a5f70cea817803aaadfbd242803eede8f0d22cbd4eee632ee2589e7ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
